{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Uncomment the following lines if you want to use the freestylo package\n",
    "'''\n",
    "from freestylo import TextObject\n",
    "from freestylo import TextPreprocessor\n",
    "from freestylo.AlliterationAnnotation import AlliterationAnnotation\n",
    "from freestylo.EpiphoraAnnotation import EpiphoraAnnotation\n",
    "from freestylo.PolysyndetonAnnotation import PolysyndetonAnnotation\n",
    "'''\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import log2\n",
    "import re\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb209f",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e904a5",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f155e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hadrienstrichard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66619d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_sm\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5d5d3",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35aa90c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Excerpt_ID</th>\n",
       "      <th>Excerpt_Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_1</td>\n",
       "      <td>n'est souvent déterminée que par un mot. En ce...</td>\n",
       "      <td>n'est souvent déterminée que par un mot. en ce...</td>\n",
       "      <td>[n'est, souvent, déterminée, que, par, un, mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_2</td>\n",
       "      <td>Le reste ne me regarde point. J'ai dit de qui ...</td>\n",
       "      <td>le reste ne me regarde point. j'ai dit de qui ...</td>\n",
       "      <td>[le, reste, ne, me, regarde, point, ., j'ai, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_3</td>\n",
       "      <td>Les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>[les, sylphes, ,, tout, étourdis, du, bruit, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_4</td>\n",
       "      <td>À peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>à peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>[à, peine, mes, yeux, sont, fermés, ,, à, pein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_5</td>\n",
       "      <td>C'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>c'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>[c'est, en, vain, que, le, jour, s'éteindrait,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Author                                              Title  \\\n",
       "index                                                                      \n",
       "0      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "1      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "2      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "3      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "4      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "\n",
       "                                          URL Excerpt_ID  \\\n",
       "index                                                      \n",
       "0      https://www.gutenberg.org/ebooks/18083    18083_1   \n",
       "1      https://www.gutenberg.org/ebooks/18083    18083_2   \n",
       "2      https://www.gutenberg.org/ebooks/18083    18083_3   \n",
       "3      https://www.gutenberg.org/ebooks/18083    18083_4   \n",
       "4      https://www.gutenberg.org/ebooks/18083    18083_5   \n",
       "\n",
       "                                            Excerpt_Text  \\\n",
       "index                                                      \n",
       "0      n'est souvent déterminée que par un mot. En ce...   \n",
       "1      Le reste ne me regarde point. J'ai dit de qui ...   \n",
       "2      Les sylphes, tout étourdis du bruit de la veil...   \n",
       "3      À peine mes yeux sont fermés, à peine cesse la...   \n",
       "4      C'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                            Cleaned_Text  \\\n",
       "index                                                      \n",
       "0      n'est souvent déterminée que par un mot. en ce...   \n",
       "1      le reste ne me regarde point. j'ai dit de qui ...   \n",
       "2      les sylphes, tout étourdis du bruit de la veil...   \n",
       "3      à peine mes yeux sont fermés, à peine cesse la...   \n",
       "4      c'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                                  Tokens  \n",
       "index                                                     \n",
       "0      [n'est, souvent, déterminée, que, par, un, mot...  \n",
       "1      [le, reste, ne, me, regarde, point, ., j'ai, d...  \n",
       "2      [les, sylphes, ,, tout, étourdis, du, bruit, d...  \n",
       "3      [à, peine, mes, yeux, sont, fermés, ,, à, pein...  \n",
       "4      [c'est, en, vain, que, le, jour, s'éteindrait,...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_parquet('../Data/excerpts_processed.parquet')\n",
    "\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd8481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Author        14722 non-null  object\n",
      " 1   Title         14722 non-null  object\n",
      " 2   URL           14722 non-null  object\n",
      " 3   Excerpt_ID    14722 non-null  object\n",
      " 4   Excerpt_Text  14722 non-null  object\n",
      " 5   Cleaned_Text  14722 non-null  object\n",
      " 6   Tokens        14722 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 920.1+ KB\n"
     ]
    }
   ],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081f503",
   "metadata": {},
   "source": [
    "## Function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5526b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of French function words: 157\n",
      "Sample function words: ['ses', 'eux', 'seront', 'ma', 'mes', 'la', 'est', 'eu', 'auriez', 'notre']\n"
     ]
    }
   ],
   "source": [
    "# Load French function words (common stopwords in French) as our function word list.\n",
    "french_function_words = set(stopwords.words('french'))\n",
    "print(f\"Number of French function words: {len(french_function_words)}\")\n",
    "# Optionally, inspect a few function words\n",
    "print(\"Sample function words:\", list(french_function_words)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of punctuation marks we will consider for certain features.\n",
    "punct_set = {'.', ',', ';', '!', '?', ':', '-', '—', '(', ')', '[', ']', '{', '}', '\"', \"'\", '«', '»'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3eb410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author       Charles Nodier\n",
      "fw_ses             0.001214\n",
      "fw_eux                  0.0\n",
      "fw_seront               0.0\n",
      "fw_ma              0.003643\n",
      "fw_mes             0.006072\n",
      "Name: 0, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 165 entries, Author to fw_aviez\n",
      "dtypes: float64(157), int64(1), object(7)\n",
      "memory usage: 19.1+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n"
     ]
    }
   ],
   "source": [
    "# Compute total number of words (excluding punctuation) for each text, to use in normalization\n",
    "texts['WordCount'] = texts['Tokens'].apply(lambda tokens: sum(1 for t in tokens if t not in punct_set))\n",
    "\n",
    "# Add one feature column for each function word in the list\n",
    "for fw in french_function_words:\n",
    "    col_name = f\"fw_{fw}\"\n",
    "    # Normalized frequency: count of fw divided by number of words (avoid division by zero)\n",
    "    texts[col_name] = texts.apply(\n",
    "        lambda row: row['Tokens'].count(fw) / row['WordCount'] if row['WordCount'] > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Example: check a few function word feature columns for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"fw_{w}\" for w in list(french_function_words)[:5]]])\n",
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f985ff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/1679231258.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/1679231258.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/1679231258.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/1679231258.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfw_article        0.112325\n",
      "gfw_preposition    0.105647\n",
      "gfw_pronoun        0.078931\n",
      "gfw_conjunction    0.063145\n",
      "gfw_auxiliary      0.001214\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_48389/1679231258.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n"
     ]
    }
   ],
   "source": [
    "# Define function word groups\n",
    "func_word_groups = {\n",
    "    \"gfw_article\": {\"le\", \"la\", \"les\", \"un\", \"une\", \"du\", \"des\"},\n",
    "    \"gfw_preposition\": {\"à\", \"de\", \"en\", \"dans\", \"avec\", \"sans\", \"sous\", \"sur\", \"chez\"},\n",
    "    \"gfw_pronoun\": {\"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \"on\", \"lui\", \"leur\", \"se\", \"me\", \"te\", \"moi\", \"toi\", \"qui\", \"que\", \"quoi\"},\n",
    "    \"gfw_conjunction\": {\"et\", \"ou\", \"mais\", \"donc\", \"or\", \"car\", \"puisque\", \"lorsque\", \"parce\", \"que\", \"si\"},\n",
    "    \"gfw_auxiliary\": {\"être\", \"avoir\"}\n",
    "}\n",
    "\n",
    "# Compute per-group normalized frequencies\n",
    "for group_name, word_set in func_word_groups.items():\n",
    "    texts[group_name] = texts.apply(\n",
    "        lambda row: sum(row['Tokens'].count(w) for w in word_set) / row['WordCount']\n",
    "        if row['WordCount'] > 0 else 0.0, axis=1\n",
    "    )\n",
    "\n",
    "# Example: check the computed features for the first text\n",
    "print(texts.loc[0, ['gfw_article', 'gfw_preposition', 'gfw_pronoun', 'gfw_conjunction', 'gfw_auxiliary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e80b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 170 entries, Author to gfw_auxiliary\n",
      "dtypes: float64(162), int64(1), object(7)\n",
      "memory usage: 19.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe22b",
   "metadata": {},
   "source": [
    "## Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a57413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define POS tags of interest\n",
    "pos_tags = ['NOUN', 'PROPN', 'VERB', 'AUX', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'CCONJ', 'SCONJ', 'INTJ']\n",
    "\n",
    "# Initialize POS frequency containers\n",
    "pos_freqs = defaultdict(list)\n",
    "\n",
    "for tokens in texts['Tokens']:\n",
    "    doc = nlp_fr(\" \".join(tokens))\n",
    "    total_words = sum(1 for token in doc if token.pos_ != \"PUNCT\")\n",
    "    \n",
    "    pos_counts = Counter(token.pos_ for token in doc if token.pos_ != \"PUNCT\")\n",
    "    pos_counts['NOUN'] += pos_counts.get('PROPN', 0)  # Merge PROPN into NOUN\n",
    "    for pos in pos_tags:\n",
    "        freq = pos_counts.get(pos, 0) / total_words if total_words > 0 else 0.0\n",
    "        pos_freqs[pos].append(freq)\n",
    "\n",
    "# Assign POS frequency features to the DataFrame\n",
    "for pos in pos_tags:\n",
    "    col = 'POS_' + ('NOUN' if pos == 'PROPN' else pos)\n",
    "    if col not in texts.columns:\n",
    "        texts[col] = pos_freqs[pos]\n",
    "        \n",
    "# Example: view the POS distribution for the first text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed414d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author       Charles Nodier\n",
      "POS_NOUN           0.195108\n",
      "POS_VERB           0.113952\n",
      "POS_AUX            0.037799\n",
      "POS_ADJ            0.081156\n",
      "POS_ADV            0.085047\n",
      "POS_PRON           0.117843\n",
      "POS_DET            0.141745\n",
      "POS_ADP            0.156754\n",
      "POS_CCONJ          0.033908\n",
      "POS_SCONJ          0.033352\n",
      "POS_INTJ                0.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(texts.loc[0, ['Author'] + [f\"POS_{w}\" for w in pos_tags if w != 'PROPN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace4dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 181 entries, Author to POS_INTJ\n",
      "dtypes: float64(173), int64(1), object(7)\n",
      "memory usage: 20.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928292d0",
   "metadata": {},
   "source": [
    "## Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ecb6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author             Charles Nodier\n",
      "WordCount                    1647\n",
      "lex_TTR                  0.448087\n",
      "lex_Hapax_Ratio          0.327869\n",
      "lex_AvgWordLen           4.870674\n",
      "Name: 0, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 184 entries, Author to lex_AvgWordLen\n",
      "dtypes: float64(176), int64(1), object(7)\n",
      "memory usage: 21.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def compute_lexical_metrics(token_list):\n",
    "    # Remove punctuation and non-word tokens from the list\n",
    "    words = []\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            continue  # skip punctuation tokens\n",
    "        # Remove any punctuation attached to the token (e.g., apostrophes or hyphens) for word length calculation\n",
    "        cleaned_token = re.sub(r'[\\W_]+', '', token, flags=re.UNICODE)  # keep only alphanumeric characters\n",
    "        if cleaned_token == \"\" or cleaned_token.isdigit():\n",
    "            continue  # skip if token is empty or purely numeric after cleaning\n",
    "        words.append(cleaned_token)\n",
    "    if len(words) == 0:\n",
    "        # If no valid word tokens, return zeros\n",
    "        return pd.Series({\"lex_TTR\": 0.0, \"lex_Hapax_Ratio\": 0.0, \"lex_AvgWordLen\": 0.0})\n",
    "    total_words = len(words)\n",
    "    unique_words = set(words)\n",
    "    # Type-Token Ratio\n",
    "    ttr = len(unique_words) / total_words\n",
    "    # Hapax Legomena Ratio\n",
    "    word_freqs = Counter(words)\n",
    "    hapax_count = sum(1 for count in word_freqs.values() if count == 1)\n",
    "    hapax_ratio = hapax_count / total_words\n",
    "    # Average word length (in characters)\n",
    "    total_chars = sum(len(w) for w in words)\n",
    "    avg_word_len = total_chars / total_words\n",
    "    return pd.Series({\"lex_TTR\": ttr, \"lex_Hapax_Ratio\": hapax_ratio, \"lex_AvgWordLen\": avg_word_len})\n",
    "\n",
    "# Apply the lexical metrics function to each text\n",
    "lexical_df = texts['Tokens'].apply(compute_lexical_metrics)\n",
    "# Merge the resulting metrics columns into the main DataFrame\n",
    "texts = pd.concat([texts, lexical_df], axis=1)\n",
    "\n",
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['Author', 'WordCount', 'lex_TTR', 'lex_Hapax_Ratio', 'lex_AvgWordLen']])\n",
    "print(texts.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b67529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_extended_lexical_metrics(token_list):\n",
    "    words = []\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            continue\n",
    "        cleaned_token = re.sub(r'[\\W_]+', '', token, flags=re.UNICODE)\n",
    "        if cleaned_token == \"\" or cleaned_token.isdigit():\n",
    "            continue\n",
    "        words.append(cleaned_token)\n",
    "\n",
    "    if len(words) == 0:\n",
    "        return pd.Series({\n",
    "            \"lex_YuleK\": 0.0,\n",
    "            \"lex_SimpsonD\": 0.0,\n",
    "            \"lex_Entropy\": 0.0\n",
    "        })\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    total = sum(word_counts.values())\n",
    "    M1 = sum(v**2 for v in word_counts.values())\n",
    "    yule_K = 10**4 * (M1 - total) / total**2\n",
    "    simpson_D = sum((freq / total)**2 for freq in word_counts.values())\n",
    "    entropy = -sum((freq / total) * log2(freq / total) for freq in word_counts.values())\n",
    "\n",
    "    return pd.Series({\n",
    "        \"lex_YuleK\": yule_K,\n",
    "        \"lex_SimpsonD\": simpson_D,\n",
    "        \"lex_Entropy\": entropy\n",
    "    })\n",
    "\n",
    "texts = texts.rename(columns={\"WordCount\": \"lex_WordCount\"})\n",
    "texts = pd.concat([texts, texts['Tokens'].apply(compute_extended_lexical_metrics)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['Author', 'lex_WordCount', 'lex_YuleK', 'lex_SimpsonD', 'lex_Entropy']])\n",
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560d1b7",
   "metadata": {},
   "source": [
    "## Syntaxic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeedc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_syntaxic_features(token_list):\n",
    "    doc = nlp_fr(\" \".join(token_list))\n",
    "    pos_counts = Counter(token.pos_ for token in doc if token.pos_ != \"PUNCT\")\n",
    "    nouns = pos_counts.get(\"NOUN\", 0) + pos_counts.get(\"PROPN\", 0)\n",
    "    verbs = pos_counts.get(\"VERB\", 0)\n",
    "    adjs = pos_counts.get(\"ADJ\", 0)\n",
    "    advs = pos_counts.get(\"ADV\", 0)\n",
    "\n",
    "    syn_nom_verb = nouns / verbs if verbs > 0 else 0.0\n",
    "    syn_adj_noun = adjs / nouns if nouns > 0 else 0.0\n",
    "    syn_adv_verb = advs / verbs if verbs > 0 else 0.0\n",
    "\n",
    "    sentence_lengths = [len([tok for tok in sent if not tok.is_punct]) for sent in doc.sents]\n",
    "    complexity_deps = {\"ccomp\", \"xcomp\", \"advcl\", \"relcl\", \"acl\", \"mark\"}\n",
    "    complexity_score = sum(1 for tok in doc if tok.dep_ in complexity_deps) / len(sentence_lengths) if sentence_lengths else 0.0\n",
    "\n",
    "    avg_len = np.mean(sentence_lengths) if sentence_lengths else 0.0\n",
    "    alternance_pattern = [1 if l > avg_len else 0 for l in sentence_lengths]\n",
    "    switches = sum(1 for i in range(1, len(alternance_pattern)) if alternance_pattern[i] != alternance_pattern[i-1])\n",
    "    alternance_rate = switches / len(alternance_pattern) if alternance_pattern else 0.0\n",
    "\n",
    "    return pd.Series({\n",
    "        \"syn_Complexity\": complexity_score,\n",
    "        \"syn_NomVerbRatio\": syn_nom_verb,\n",
    "        \"syn_AdjNounRatio\": syn_adj_noun,\n",
    "        \"syn_AdvVerbRatio\": syn_adv_verb,\n",
    "        \"syn_AlternanceRate\": alternance_rate\n",
    "    })\n",
    "\n",
    "texts = pd.concat([texts, texts['Tokens'].apply(compute_syntaxic_features)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28412c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['Author', 'syn_Complexity', 'syn_NomVerbRatio', 'syn_AdjNounRatio', 'syn_AdvVerbRatio', 'syn_AlternanceRate']])\n",
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36637095",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd3a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['pun_Period_Count', 'pun_Comma_Count', 'pun_Semicolon_Count', 'pun_QuestionMark_Count', 'pun_ExclamationMark_Count', 'pun_Colon_Count'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     texts[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_Count\u001b[39m\u001b[33m\"\u001b[39m] = texts[\u001b[33m'\u001b[39m\u001b[33mTokens\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m tokens: tokens.count(symbol))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Example: show punctuation counts for the first text\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtexts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAuthor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpun_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_Count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpunct_marks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1184\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1187\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1368\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[32m   1367\u001b[39m     tup = \u001b[38;5;28mself\u001b[39m._expand_ellipsis(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[32m   1371\u001b[39m tup = \u001b[38;5;28mself\u001b[39m._validate_tuple_indexer(tup)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1089\u001b[39m, in \u001b[36m_LocationIndexer._getitem_lowerdim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1087\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[32m   1088\u001b[39m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[33m\"\u001b[39m\u001b[33mnot applicable\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1360\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1359\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1362\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1363\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexing.py:1558\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1555\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1556\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['pun_Period_Count', 'pun_Comma_Count', 'pun_Semicolon_Count', 'pun_QuestionMark_Count', 'pun_ExclamationMark_Count', 'pun_Colon_Count'] not in index\""
     ]
    }
   ],
   "source": [
    "# Define a mapping of punctuation symbols to descriptive column names\n",
    "punct_marks = {'.': 'Period', ',': 'Comma', ';': 'Semicolon', '?': 'QuestionMark', '!': 'ExclamationMark', ':': 'Colon'}\n",
    "\n",
    "for symbol, name in punct_marks.items():\n",
    "    texts[f\"pun_{name}_Count\"] = texts['Tokens'].apply(lambda tokens: tokens.count(symbol))\n",
    "\n",
    "# Example: show punctuation counts for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"pun_{name}_Count\" for name in punct_marks.values()]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a5ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Author', 'Title', 'URL', 'Excerpt_ID', 'Excerpt_Text', 'Cleaned_Text',\n",
      "       'Tokens', 'WordCount', 'fw_ses', 'fw_eux',\n",
      "       ...\n",
      "       'POS_INTJ', 'lex_TTR', 'lex_Hapax_Ratio', 'lex_AvgWordLen',\n",
      "       'pun_Period_Count', 'pun_Comma_Count', 'pun_Semicolon_Count',\n",
      "       'pun_QuestionMark_Count', 'pun_ExclamationMark_Count',\n",
      "       'pun_Colon_Count'],\n",
      "      dtype='object', length=190)\n"
     ]
    }
   ],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84622446",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dep_features(token_list):\n",
    "    doc = nlp_fr(\" \".join(token_list))\n",
    "    deps = {\"ccomp\", \"xcomp\", \"advcl\", \"relcl\", \"acl\", \"mark\"}\n",
    "    dep_counts = Counter(token.dep_ for token in doc)\n",
    "    return pd.Series({f\"dep_{d}\": dep_counts.get(d, 0) for d in deps})\n",
    "\n",
    "texts = pd.concat([texts, texts['Tokens'].apply(compute_dep_features)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['Author', 'dep_ccomp', 'dep_xcomp', 'dep_advcl', 'dep_relcl', 'dep_acl', 'dep_mark']])\n",
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148761dc",
   "metadata": {},
   "source": [
    "## Sentence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac16862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_metrics_with_complexity(token_list):\n",
    "    # Count sentence-ending punctuation marks\n",
    "    sentence_enders = {'.', '!', '?'}\n",
    "    \n",
    "    # Count words (exclude all punctuation tokens)\n",
    "    sent_count = sum(token_list.count(sym) for sym in sentence_enders)\n",
    "    sent_count = max(sent_count, 1 if any(t not in punct_set for t in token_list) else 0)\n",
    "    word_count = sum(1 for t in token_list if t not in punct_set)\n",
    "    avg_sent_len = word_count / sent_count if sent_count > 0 else 0.0\n",
    "\n",
    "\n",
    "    return pd.Series({\n",
    "        \"sent_Count\": sent_count,\n",
    "        \"sent_AvgLength\": avg_sent_len\n",
    "    })\n",
    "\n",
    "sentence_df = texts['Tokens'].apply(compute_sentence_metrics_with_complexity)\n",
    "texts = pd.concat([texts, sentence_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5adb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_variation_features(token_list):\n",
    "    doc = nlp_fr(\" \".join(token_list))\n",
    "    sentences = list(doc.sents)\n",
    "    sentence_lengths = [len([tok for tok in sent if not tok.is_punct]) for sent in sentences]\n",
    "    sent_count = len(sentences)\n",
    "    total_words = sum(sentence_lengths)\n",
    "\n",
    "    avg_len = total_words / sent_count if sent_count > 0 else 0.0\n",
    "    std_len = np.std(sentence_lengths) if sentence_lengths else 0.0\n",
    "    min_len = min(sentence_lengths) if sentence_lengths else 0\n",
    "    max_len = max(sentence_lengths) if sentence_lengths else 0\n",
    "\n",
    "    passive_count = sum(\n",
    "        1 for tok in doc\n",
    "        if tok.dep_ == \"aux:pass\" or \n",
    "           (tok.head.lemma_ == \"être\" and tok.pos_ == \"VERB\" and \"VerbForm=Part\" in tok.morph)\n",
    "    )\n",
    "\n",
    "    intj_start = sum(1 for sent in sentences if len(sent) > 0 and sent[0].pos_ == \"INTJ\")\n",
    "    questions = sum(1 for sent in sentences if any(tok.text == \"?\" for tok in sent))\n",
    "    exclamations = sum(1 for sent in sentences if any(tok.text == \"!\" for tok in sent))\n",
    "\n",
    "    return pd.Series({\n",
    "        \"sent_MinLength\": min_len,\n",
    "        \"sent_MaxLength\": max_len,\n",
    "        \"sent_StdLength\": std_len,\n",
    "        \"sent_PassiveRatio\": passive_count / sent_count if sent_count > 0 else 0.0,\n",
    "        \"sent_Interrogative\": questions / sent_count if sent_count > 0 else 0.0,\n",
    "        \"sent_Exclamative\": exclamations / sent_count if sent_count > 0 else 0.0,\n",
    "        \"sent_Exclamative_INTJ\": intj_start / sent_count if sent_count > 0 else 0.0\n",
    "    })\n",
    "\n",
    "texts = pd.concat([texts, texts['Tokens'].apply(compute_sentence_variation_features)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839461a",
   "metadata": {},
   "source": [
    "## Verb Tenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4aeb671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tense/mood patterns to track\n",
    "tense_patterns = {\n",
    "    \"tense_pres_ind\": (\"Tense=Pres\", \"Mood=Ind\"),\n",
    "    \"tense_past_ind\": (\"Tense=Past\", \"Mood=Ind\"),\n",
    "    \"tense_imp_ind\": (\"Tense=Imp\", \"Mood=Ind\"),\n",
    "    \"tense_cond\": (\"Mood=Cnd\",),\n",
    "    \"tense_subj\": (\"Mood=Sub\",),\n",
    "    \"tense_inf\": (\"VerbForm=Inf\",)\n",
    "}\n",
    "\n",
    "# Initialize dict of lists\n",
    "tense_counts = {k: [] for k in tense_patterns.keys()}\n",
    "\n",
    "for tokens in texts['Tokens']:\n",
    "    doc = nlp_fr(\" \".join(tokens))\n",
    "    verb_tokens = [t for t in doc if t.pos_ in {\"VERB\", \"AUX\"}]\n",
    "    total_verbs = len(verb_tokens)\n",
    "    \n",
    "    for label, conditions in tense_patterns.items():\n",
    "        count = sum(\n",
    "            all(cond in t.morph.to_json() for cond in conditions)\n",
    "            for t in verb_tokens\n",
    "        )\n",
    "        tense_counts[label].append(count / total_verbs if total_verbs > 0 else 0.0)\n",
    "\n",
    "# Add to DataFrame\n",
    "for label, vals in tense_counts.items():\n",
    "    texts[label] = vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c0f9247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 186 entries, Author to tense_inf\n",
      "dtypes: float64(178), int64(7), object(1)\n",
      "memory usage: 21.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2dc03",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_conjs = {\"et\", \"ou\", \"mais\", \"ni\", \"donc\", \"or\", \"car\"}\n",
    "subord_conjs = {\"que\", \"quand\", \"si\", \"comme\", \"lorsque\", \"puisque\", \"bien que\", \"afin que\", \"parce que\"}\n",
    "discursive_markers = {\"donc\", \"cependant\", \"toutefois\", \"néanmoins\", \"ainsi\", \"en effet\", \"or\", \"en revanche\", \"par conséquent\", \"en outre\"}\n",
    "\n",
    "def compute_discursive_features(token_list):\n",
    "    coord_count = sum(1 for t in token_list if t.lower() in coord_conjs)\n",
    "    subord_count = sum(1 for t in token_list if t.lower() in subord_conjs)\n",
    "    disc_count = sum(1 for t in token_list if t.lower() in discursive_markers)\n",
    "    ratio = subord_count / (coord_count + 1)\n",
    "    return pd.Series({\n",
    "        \"syn_CoordCount\": coord_count,\n",
    "        \"syn_SubordCount\": subord_count,\n",
    "        \"syn_SubCoordRatio\": ratio,\n",
    "        \"lex_MarkerCount\": disc_count\n",
    "    })\n",
    "\n",
    "texts = pd.concat([texts, texts['Tokens'].apply(compute_discursive_features)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b91c31",
   "metadata": {},
   "source": [
    "## Character Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total character n-gram features: 28205\n",
      "bi_n'     20.0\n",
      "bi_'e     37.0\n",
      "bi_es    206.0\n",
      "bi_st     32.0\n",
      "bi_t_    201.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Commented out as it added too much features for too little gain\n",
    "\n",
    "'''\n",
    "\n",
    "def reconstruct_text(token_list):\n",
    "    \"\"\"Reconstruct text string from tokens, inserting spaces appropriately.\"\"\"\n",
    "    text = \"\"\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            # Attach punctuation directly to the text (no preceding space)\n",
    "            text += token\n",
    "        else:\n",
    "            # If not the first token, add a space before adding the word\n",
    "            if text:\n",
    "                text += \" \"\n",
    "            text += token\n",
    "    return text\n",
    "\n",
    "# List to collect n-gram frequency dictionaries for each text\n",
    "char_ngram_features = []\n",
    "\n",
    "for token_list in texts['Tokens']:\n",
    "    text = reconstruct_text(token_list)\n",
    "    # Initialize a dict for this text's n-grams\n",
    "    ngram_counts = {}\n",
    "    # Character bigrams (2-grams)\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i:i+2]\n",
    "        # Use an underscore to represent spaces in feature names for clarity\n",
    "        feat_name = \"bi_\" + bigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    # Character trigrams (3-grams)\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i+3]\n",
    "        feat_name = \"tri_\" + trigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    char_ngram_features.append(ngram_counts)\n",
    "\n",
    "# Convert list of dicts to DataFrame (each n-gram becomes a column, fill missing with 0)\n",
    "char_ngram_df = pd.DataFrame(char_ngram_features).fillna(0).astype(int)\n",
    "# Merge the n-gram features into the main DataFrame\n",
    "texts = pd.concat([texts, char_ngram_df], axis=1)\n",
    "\n",
    "# Example: number of character n-gram feature columns\n",
    "print(f\"Total character n-gram features: {char_ngram_df.shape[1]}\")\n",
    "# Show a small sample of character n-gram features for the first text\n",
    "print(texts.loc[0, [col for col in texts.columns if col.startswith('bi_')][:5]])\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2566b",
   "metadata": {},
   "source": [
    "## Figure of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44caa73c",
   "metadata": {},
   "source": [
    "Avec freestylo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out as it made the computer freeze\n",
    "\n",
    "'''\n",
    "\n",
    "FIGURES = [AlliterationAnnotation,\n",
    "           EpiphoraAnnotation, PolysyndetonAnnotation]\n",
    "\n",
    "def extract_stylistic_features(text, lang='fr', nlp=None):\n",
    "    tobj = TextObject.TextObject(text=text, language=lang)\n",
    "    tp = TextPreprocessor.TextPreprocessor(language=lang)\n",
    "    tp.nlp = nlp\n",
    "    tp.process_text(tobj)\n",
    "\n",
    "    features, examples = {}, {}\n",
    "    for AnnotClass in FIGURES:\n",
    "        annot = AnnotClass(text=tobj)\n",
    "        annot.find_candidates()\n",
    "        if hasattr(annot, 'load_model'):\n",
    "            annot.load_model()\n",
    "        if hasattr(annot, 'score_candidates'):\n",
    "            annot.score_candidates()\n",
    "\n",
    "        instances = annot.candidates \n",
    "        features[f\"fig_{AnnotClass.__name__}\"] = len(instances)\n",
    "        examples[AnnotClass.__name__] = instances\n",
    "    return features, examples\n",
    "\n",
    "\n",
    "def apply_stylistic_features(df, column='Cleaned_Text', nlp=None):\n",
    "    feats_list, ex_list = [], []\n",
    "\n",
    "    for txt in tqdm(df[column].astype(str), desc=\"Extracting stylistic features\"):\n",
    "        feats, ex = extract_stylistic_features(txt, nlp=nlp)\n",
    "        feats_list.append(feats)\n",
    "        ex_list.append(ex)\n",
    "\n",
    "    feats_df = pd.DataFrame(feats_list).fillna(0).astype(int)\n",
    "    return pd.concat([df.reset_index(drop=True), feats_df], axis=1), ex_list\n",
    "    \n",
    "texts, figure_examples = apply_stylistic_features(texts, nlp=nlp_fr)\n",
    "\n",
    "texts['fig_AlliterationAnnotation', 'fig_EpiphoraAnnotation', 'fig_PolysyndetonAnnotation'].describe()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3f82e",
   "metadata": {},
   "source": [
    "LLM labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77d534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_figures(phrase: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "Tu es un expert en analyse littéraire. Analyse la phrase suivante et indique quelles figures de style elle contient, en te basant sur cette liste. Pour chaque figure, réponds par vrai ou faux. Voici les figures à analyser, avec leur définition et un exemple pour chacune :\n",
    "\n",
    "1. **Comparaison** : Mise en relation d’un comparé et d’un comparant avec un outil (comme, tel, semblable à, etc.).\n",
    "   Exemple : « Cet homme est bête comme ses pieds »\n",
    "2. **Métaphore** : Comparaison sans outil de comparaison.\n",
    "   Exemple : « Ses cheveux de miel. »\n",
    "3. **Personnification** : Attribution de caractéristiques humaines à un objet ou un animal.\n",
    "   Exemple : « Le stylo saute de la table »\n",
    "4. **Hyperbole** : Exagération dans le but de frapper l’imagination.\n",
    "   Exemple : « Je meurs de faim »\n",
    "5. **Litote** : Dire moins pour faire entendre plus.\n",
    "   Exemple : « Je ne te hais point »\n",
    "6. **Antithèse** : Opposition très forte entre deux idées.\n",
    "   Exemple : « Je vis, je meurs »\n",
    "7. **Oxymore** : Réunion de deux termes opposés dans un même groupe de mots.\n",
    "   Exemple : « Une obscure clarté »\n",
    "8. **Répétition** : Reprise d’un même mot plusieurs fois.\n",
    "   Exemple : « La guerre, la guerre, la guerre ! »\n",
    "9. **Énumération** : Accumulation de mots de même nature grammaticale.\n",
    "   Exemple : « Il mange des pommes, des poires, des bananes et des kiwis. »\n",
    "10. **Euphémisme** : Expression atténuée d’une idée jugée brutale.\n",
    "   Exemple : « Il nous a quittés » (pour \"il est mort\")\n",
    "\n",
    "Phrase à analyser :\n",
    "« {phrase} »\n",
    "\n",
    "Réponds uniquement avec un dictionnaire JSON contenant ces clés : \"Comparaison\", \"Métaphore\", \"Personnification\", \"Hyperbole\", \"Litote\", \"Antithèse\", \"Oxymore\", \"Répétition\", \"Énumération\", \"Euphémisme\", et pour chaque clé, la valeur `true` ou `false` (sans guillemets).\n",
    "\n",
    "Ne donne pas d’explication.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Extraction de la sortie générée\n",
    "    generated = response.choices[0].message.content\n",
    "\n",
    "    # Évaluation sécurisée du JSON renvoyé (sans `eval`)\n",
    "    import json\n",
    "    try:\n",
    "        result = json.loads(generated)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Parsing error:\", generated)\n",
    "        result = {}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "011d07eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Comparaison': False,\n",
       " 'Métaphore': False,\n",
       " 'Personnification': False,\n",
       " 'Hyperbole': False,\n",
       " 'Litote': True,\n",
       " 'Antithèse': False,\n",
       " 'Oxymore': False,\n",
       " 'Répétition': False,\n",
       " 'Énumération': False,\n",
       " 'Euphémisme': False}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_figures('Je ne te hais point')  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "608402ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running LLM predictions: 100%|██████████| 28/28 [00:40<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (relaxed): 96.43%\n",
      "\n",
      "Total errors: 1 / 28\n",
      "\n",
      "---\n",
      "Sentence      : Candidese cacha du mieux qu’il put pendant cette boucherie héroïque.\n",
      "True Label    : Oxymore\n",
      "LLM Prediction: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM labeling function\n",
    "\n",
    "# Load the test dataset\n",
    "fig_df = pd.read_csv(\n",
    "    \"../Data/figures.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\";\",\n",
    ")\n",
    "print(len(fig_df))\n",
    "\n",
    "# Normalize figure labels\n",
    "fig_df[\"Figure\"] = fig_df[\"Figure\"].str.strip().str.capitalize()\n",
    "\n",
    "# Store results\n",
    "predicted_labels = []\n",
    "correct_flags = []\n",
    "\n",
    "# Loop through each sentence and evaluate\n",
    "for i, row in tqdm(fig_df.iterrows(), total=len(fig_df), desc=\"Running LLM predictions\"):\n",
    "    sentence = row[\"Texte\"]\n",
    "    true_label = row[\"Figure\"]\n",
    "\n",
    "    result = detect_figures(sentence)\n",
    "\n",
    "    # Store full prediction dictionary (optional)\n",
    "    predicted_labels.append(result)\n",
    "\n",
    "    # Check if the true label is among the ones flagged as True\n",
    "    detected_true = result.get(true_label, False)\n",
    "    correct_flags.append(detected_true)\n",
    "\n",
    "# Add evaluation columns\n",
    "fig_df[\"LLM_Output\"] = predicted_labels\n",
    "fig_df[\"Correct\"] = correct_flags\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = fig_df[\"Correct\"].mean()\n",
    "print(f\"\\nAccuracy (relaxed): {accuracy:.2%}\")\n",
    "\n",
    "# Show errors (where the true label was not among the detected ones)\n",
    "errors_df = fig_df[~fig_df[\"Correct\"]]\n",
    "print(f\"\\nTotal errors: {len(errors_df)} / {len(fig_df)}\")\n",
    "\n",
    "# Display mismatches\n",
    "for idx, row in errors_df.iterrows():\n",
    "    print(\"\\n---\")\n",
    "    print(f\"Sentence      : {row['Texte']}\")\n",
    "    print(f\"True Label    : {row['Figure']}\")\n",
    "    print(f\"LLM Prediction: {[k for k, v in row['LLM_Output'].items() if v]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ccd0366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hadrienstrichard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac5c623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target figures of speech\n",
    "FIGURES = [\n",
    "    \"Comparaison\", \"Métaphore\", \"Personnification\", \"Hyperbole\", \"Litote\",\n",
    "    \"Antithèse\", \"Oxymore\", \"Répétition\", \"Énumération\", \"Euphémisme\"\n",
    "]\n",
    "\n",
    "def detect_figures_in_text(text, detection_function):\n",
    "    \"\"\"\n",
    "    Tokenize the text into sentences and apply the detection function to each.\n",
    "    Returns a dictionary of counts for each figure.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text, language='french')\n",
    "    counts = {f\"fig_{fig}\": 0 for fig in FIGURES}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        result = detection_function(sentence)\n",
    "        for fig, detected in result.items():\n",
    "            if detected:\n",
    "                counts[f\"fig_{fig}\"] += 1\n",
    "    return counts\n",
    "\n",
    "def add_figure_counts_to_df(df, detection_function, text_column=\"Cleaned_Text\"):\n",
    "    \"\"\"\n",
    "    Applies figure detection to each row of the DataFrame and adds new feature columns.\n",
    "    Each new column represents the number of times a figure was detected in that text.\n",
    "    \"\"\"\n",
    "    tqdm.pandas()  # for progress bar\n",
    "    figure_counts = df[text_column].progress_apply(lambda text: detect_figures_in_text(text, detection_function))\n",
    "    figure_df = pd.DataFrame(list(figure_counts))\n",
    "    return pd.concat([df, figure_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c36c2101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:03<00:00, 63.01s/it]\n"
     ]
    }
   ],
   "source": [
    "df_ex = add_figure_counts_to_df(texts[:1], detect_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b6e9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Excerpt_ID</th>\n",
       "      <th>Excerpt_Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>fig_Comparaison</th>\n",
       "      <th>fig_Métaphore</th>\n",
       "      <th>fig_Personnification</th>\n",
       "      <th>fig_Hyperbole</th>\n",
       "      <th>fig_Litote</th>\n",
       "      <th>fig_Antithèse</th>\n",
       "      <th>fig_Oxymore</th>\n",
       "      <th>fig_Répétition</th>\n",
       "      <th>fig_Énumération</th>\n",
       "      <th>fig_Euphémisme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_1</td>\n",
       "      <td>n'est souvent déterminée que par un mot. En ce...</td>\n",
       "      <td>n'est souvent déterminée que par un mot. en ce...</td>\n",
       "      <td>[n'est, souvent, déterminée, que, par, un, mot...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                              Title  \\\n",
       "0  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "\n",
       "                                      URL Excerpt_ID  \\\n",
       "0  https://www.gutenberg.org/ebooks/18083    18083_1   \n",
       "\n",
       "                                        Excerpt_Text  \\\n",
       "0  n'est souvent déterminée que par un mot. En ce...   \n",
       "\n",
       "                                        Cleaned_Text  \\\n",
       "0  n'est souvent déterminée que par un mot. en ce...   \n",
       "\n",
       "                                              Tokens  fig_Comparaison  \\\n",
       "0  [n'est, souvent, déterminée, que, par, un, mot...                4   \n",
       "\n",
       "   fig_Métaphore  fig_Personnification  fig_Hyperbole  fig_Litote  \\\n",
       "0              2                     1              1           4   \n",
       "\n",
       "   fig_Antithèse  fig_Oxymore  fig_Répétition  fig_Énumération  fig_Euphémisme  \n",
       "0              6            1               0               10               1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79497473",
   "metadata": {},
   "source": [
    "## Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b51cfc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 186 entries, Author to tense_inf\n",
      "dtypes: float64(178), int64(7), object(1)\n",
      "memory usage: 21.5+ MB\n"
     ]
    }
   ],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "977a4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (14722, 199)\n",
      "float64    185\n",
      "object       7\n",
      "int64        7\n",
      "Name: count, dtype: int64\n",
      "Missing values total: 0\n",
      "0.0 % of total values are missing\n"
     ]
    }
   ],
   "source": [
    "# Taille du dataset\n",
    "print(\"Shape:\", texts.shape)\n",
    "\n",
    "# Types de données\n",
    "print(texts.dtypes.value_counts())\n",
    "\n",
    "# Nombre total de valeurs manquantes\n",
    "print(\"Missing values total:\", texts.isna().sum().sum())\n",
    "\n",
    "print(texts.isna().sum().sum() / (texts.shape[0] * texts.shape[1]) * 100, \"% of total values are missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0e84805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape after column filtering: (14722, 193)\n",
      "Total remaining NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Ne garder que les features + la colonne \"Author\"\n",
    "# On supprime tout sauf \"Author\" et les colonnes de type numérique (features)\n",
    "cols_to_keep = ['Author'] + [col for col in texts.columns if col not in ['Title', 'URL', 'Excerpt_ID', 'Excerpt_Text', 'Cleaned_Text', 'Tokens'] and col != 'Author']\n",
    "texts = texts[cols_to_keep]\n",
    "\n",
    "# Vérification\n",
    "print(\"New shape after column filtering:\", texts.shape)\n",
    "\n",
    "# Étape 2 : Remplir les NaN par la moyenne PAR CLASSE (par 'Author')\n",
    "# Cela suppose que toutes les colonnes sauf 'Author' sont numériques\n",
    "feature_cols = [col for col in texts.columns if col != 'Author']\n",
    "\n",
    "# On remplit les NaN par la moyenne des valeurs pour chaque auteur\n",
    "texts[feature_cols] = texts.groupby('Author')[feature_cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Vérification post-traitement\n",
    "print(\"Total remaining NaNs:\", texts.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc819cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de colonnes entièrement nulles ou égales à zéro : 7\n",
      "Shape finale du dataset : (14722, 186)\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : supprimer les colonnes avec uniquement des NaN\n",
    "texts = texts.dropna(axis=1, how='all')\n",
    "\n",
    "# Étape 2 : supprimer les colonnes avec uniquement des zéros (sauf colonne 'Author')\n",
    "feature_cols = [col for col in texts.columns if col != 'Author']\n",
    "zero_cols = [col for col in feature_cols if (texts[col] == 0).all()]\n",
    "\n",
    "print(f\"Nombre de colonnes entièrement nulles ou égales à zéro : {len(zero_cols)}\")\n",
    "\n",
    "# Drop them\n",
    "texts.drop(columns=zero_cols, inplace=True)\n",
    "\n",
    "# Vérification\n",
    "print(\"Shape finale du dataset :\", texts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b521d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.to_parquet(\"../Data/stylo_embedds.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a36d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les préfixes avant le premier underscore\n",
    "prefixes = texts.columns.str.extract(r'^([^_]+)_')[0]\n",
    "\n",
    "# Remplacer les NaN (pas de préfixe) par \"autre\"\n",
    "prefixes = prefixes.fillna(\"autre\")\n",
    "\n",
    "# Compter les occurrences de chaque préfixe (ou \"autre\")\n",
    "prefix_counts = prefixes.value_counts()\n",
    "\n",
    "print(prefix_counts)\n",
    "\n",
    "# Filtrer les colonnes qui ne commencent PAS par \"fw_\"\n",
    "non_fw_columns = [col for col in texts.columns if not col.startswith('fw_')]\n",
    "\n",
    "# Afficher la liste\n",
    "print(non_fw_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-paper-9dnSQuhV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
