{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b9964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f155e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hadrienstrichard/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66619d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aa90c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Excerpt_ID</th>\n",
       "      <th>Excerpt_Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>of_which_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_1</td>\n",
       "      <td>n'est souvent déterminée que par un mot. En ce...</td>\n",
       "      <td>n'est souvent déterminée que par un mot. en ce...</td>\n",
       "      <td>[n'est, souvent, déterminée, que, par, un, mot...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_2</td>\n",
       "      <td>Le reste ne me regarde point. J'ai dit de qui ...</td>\n",
       "      <td>le reste ne me regarde point. j'ai dit de qui ...</td>\n",
       "      <td>[le, reste, ne, me, regarde, point, ., j'ai, d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_3</td>\n",
       "      <td>Les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>[les, sylphes, ,, tout, étourdis, du, bruit, d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_4</td>\n",
       "      <td>À peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>à peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>[à, peine, mes, yeux, sont, fermés, ,, à, pein...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_5</td>\n",
       "      <td>C'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>c'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>[c'est, en, vain, que, le, jour, s'éteindrait,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                              Title  \\\n",
       "0  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "1  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "2  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "3  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "4  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "\n",
       "                                      URL Excerpt_ID  \\\n",
       "0  https://www.gutenberg.org/ebooks/18083    18083_1   \n",
       "1  https://www.gutenberg.org/ebooks/18083    18083_2   \n",
       "2  https://www.gutenberg.org/ebooks/18083    18083_3   \n",
       "3  https://www.gutenberg.org/ebooks/18083    18083_4   \n",
       "4  https://www.gutenberg.org/ebooks/18083    18083_5   \n",
       "\n",
       "                                        Excerpt_Text  \\\n",
       "0  n'est souvent déterminée que par un mot. En ce...   \n",
       "1  Le reste ne me regarde point. J'ai dit de qui ...   \n",
       "2  Les sylphes, tout étourdis du bruit de la veil...   \n",
       "3  À peine mes yeux sont fermés, à peine cesse la...   \n",
       "4  C'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                        Cleaned_Text  \\\n",
       "0  n'est souvent déterminée que par un mot. en ce...   \n",
       "1  le reste ne me regarde point. j'ai dit de qui ...   \n",
       "2  les sylphes, tout étourdis du bruit de la veil...   \n",
       "3  à peine mes yeux sont fermés, à peine cesse la...   \n",
       "4  c'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                              Tokens Tokenized_Text  \\\n",
       "0  [n'est, souvent, déterminée, que, par, un, mot...           None   \n",
       "1  [le, reste, ne, me, regarde, point, ., j'ai, d...           None   \n",
       "2  [les, sylphes, ,, tout, étourdis, du, bruit, d...           None   \n",
       "3  [à, peine, mes, yeux, sont, fermés, ,, à, pein...           None   \n",
       "4  [c'est, en, vain, que, le, jour, s'éteindrait,...           None   \n",
       "\n",
       "  of_which_generated  \n",
       "0               None  \n",
       "1               None  \n",
       "2               None  \n",
       "3               None  \n",
       "4               None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_parquet('../Data/excerpts_df.parquet')\n",
    "\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd8481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14680 entries, 0 to 14679\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Author              14680 non-null  object\n",
      " 1   Title               14680 non-null  object\n",
      " 2   URL                 14680 non-null  object\n",
      " 3   Excerpt_ID          14680 non-null  object\n",
      " 4   Excerpt_Text        14680 non-null  object\n",
      " 5   Cleaned_Text        14680 non-null  object\n",
      " 6   Tokens              14680 non-null  object\n",
      " 7   Tokenized_Text      55 non-null     object\n",
      " 8   of_which_generated  55 non-null     object\n",
      "dtypes: object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5526b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of French function words: 157\n",
      "Sample function words: ['s', 'du', 'ayante', 'ayant', 'eurent', 'étés', 'aies', 'serez', 'l', 'avez']\n"
     ]
    }
   ],
   "source": [
    "# Load French function words (common stopwords in French) as our function word list.\n",
    "french_function_words = set(stopwords.words('french'))\n",
    "print(f\"Number of French function words: {len(french_function_words)}\")\n",
    "# Optionally, inspect a few function words\n",
    "print(\"Sample function words:\", list(french_function_words)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of punctuation marks we will consider for certain features.\n",
    "punct_set = {'.', ',', ';', '!', '?', ':'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943ab7a",
   "metadata": {},
   "source": [
    "## Function word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3eb410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author       Charles Nodier\n",
      "fw_s                    0.0\n",
      "fw_du              0.012085\n",
      "fw_ayante               0.0\n",
      "fw_ayant                0.0\n",
      "fw_eurent               0.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Compute total number of words (excluding punctuation) for each text, to use in normalization\n",
    "texts['WordCount'] = texts['Tokens'].apply(lambda tokens: sum(1 for t in tokens if t not in punct_set))\n",
    "\n",
    "# Add one feature column for each function word in the list\n",
    "for fw in french_function_words:\n",
    "    col_name = f\"fw_{fw}\"\n",
    "    # Normalized frequency: count of fw divided by number of words (avoid division by zero)\n",
    "    texts[col_name] = texts.apply(\n",
    "        lambda row: row['Tokens'].count(fw) / row['WordCount'] if row['WordCount'] > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Example: check a few function word feature columns for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"fw_{w}\" for w in list(french_function_words)[:5]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe22b",
   "metadata": {},
   "source": [
    "## Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a57413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to collect POS frequency features for each text\n",
    "pos_noun_freq = []\n",
    "pos_verb_freq = []\n",
    "pos_adj_freq = []\n",
    "\n",
    "for tokens in texts['Tokens']:\n",
    "    # Join tokens back into a text string for POS tagging\n",
    "    doc = nlp_fr(\" \".join(tokens))\n",
    "    # Count total number of word tokens (non-punctuation) in this text\n",
    "    total_words = sum(1 for token in doc if token.pos_ != \"PUNCT\")\n",
    "    # Count specific POS categories\n",
    "    noun_count = sum(1 for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"])  # treat proper nouns as nouns\n",
    "    verb_count = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    adj_count  = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
    "    # Compute relative frequencies (avoid division by zero)\n",
    "    if total_words > 0:\n",
    "        pos_noun_freq.append(noun_count / total_words)\n",
    "        pos_verb_freq.append(verb_count / total_words)\n",
    "        pos_adj_freq.append(adj_count / total_words)\n",
    "    else:\n",
    "        pos_noun_freq.append(0.0)\n",
    "        pos_verb_freq.append(0.0)\n",
    "        pos_adj_freq.append(0.0)\n",
    "\n",
    "# Add POS distribution features to the DataFrame\n",
    "texts['POS_NOUN'] = pos_noun_freq\n",
    "texts['POS_VERB'] = pos_verb_freq\n",
    "texts['POS_ADJ']  = pos_adj_freq\n",
    "\n",
    "# Example: view POS distribution for first few texts\n",
    "print(texts.loc[:4, ['Author', 'POS_NOUN', 'POS_VERB', 'POS_ADJ']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928292d0",
   "metadata": {},
   "source": [
    "## Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ecb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lexical_metrics(token_list):\n",
    "    # Remove punctuation and non-word tokens from the list\n",
    "    words = []\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            continue  # skip punctuation tokens\n",
    "        # Remove any punctuation attached to the token (e.g., apostrophes or hyphens) for word length calculation\n",
    "        cleaned_token = re.sub(r'[\\W_]+', '', token, flags=re.UNICODE)  # keep only alphanumeric characters\n",
    "        if cleaned_token == \"\" or cleaned_token.isdigit():\n",
    "            continue  # skip if token is empty or purely numeric after cleaning\n",
    "        words.append(cleaned_token)\n",
    "    if len(words) == 0:\n",
    "        # If no valid word tokens, return zeros\n",
    "        return pd.Series({\"TTR\": 0.0, \"Hapax_Ratio\": 0.0, \"AvgWordLen\": 0.0})\n",
    "    total_words = len(words)\n",
    "    unique_words = set(words)\n",
    "    # Type-Token Ratio\n",
    "    ttr = len(unique_words) / total_words\n",
    "    # Hapax Legomena Ratio\n",
    "    word_freqs = Counter(words)\n",
    "    hapax_count = sum(1 for count in word_freqs.values() if count == 1)\n",
    "    hapax_ratio = hapax_count / total_words\n",
    "    # Average word length (in characters)\n",
    "    total_chars = sum(len(w) for w in words)\n",
    "    avg_word_len = total_chars / total_words\n",
    "    return pd.Series({\"TTR\": ttr, \"Hapax_Ratio\": hapax_ratio, \"AvgWordLen\": avg_word_len})\n",
    "\n",
    "# Apply the lexical metrics function to each text\n",
    "lexical_df = texts['Tokens'].apply(compute_lexical_metrics)\n",
    "# Merge the resulting metrics columns into the main DataFrame\n",
    "texts = pd.concat([texts, lexical_df], axis=1)\n",
    "\n",
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['author', 'WordCount', 'TTR', 'Hapax_Ratio', 'AvgWordLen']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36637095",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of punctuation symbols to descriptive column names\n",
    "punct_marks = {'.': 'Period', ',': 'Comma', ';': 'Semicolon', '?': 'QuestionMark', '!': 'ExclamationMark', ':': 'Colon'}\n",
    "\n",
    "for symbol, name in punct_marks.items():\n",
    "    texts[f\"{name}_Count\"] = texts['Tokens'].apply(lambda tokens: tokens.count(symbol))\n",
    "\n",
    "# Example: show punctuation counts for the first text\n",
    "print(texts.loc[0, ['author'] + [f\"{name}_Count\" for name in punct_marks.values()]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148761dc",
   "metadata": {},
   "source": [
    "## Sentence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50330474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_metrics(token_list):\n",
    "    # Count sentence-ending punctuation marks\n",
    "    sentence_enders = {'.', '!', '?'}\n",
    "    sent_count = sum(token_list.count(sym) for sym in sentence_enders)\n",
    "    if sent_count == 0:\n",
    "        # If no explicit end markers but there are words, assume at least one sentence\n",
    "        sent_count = 1 if any(t not in punct_set for t in token_list) else 0\n",
    "    # Count words (exclude all punctuation tokens)\n",
    "    word_count = sum(1 for t in token_list if t not in punct_set)\n",
    "    avg_sent_len = word_count / sent_count if sent_count > 0 else 0.0\n",
    "    return pd.Series({\"Sentence_Count\": sent_count, \"AvgSentenceLength\": avg_sent_len})\n",
    "\n",
    "sentence_df = texts['Tokens'].apply(compute_sentence_metrics)\n",
    "texts = pd.concat([texts, sentence_df], axis=1)\n",
    "\n",
    "# Example: show sentence metrics for first few texts\n",
    "print(texts.loc[:4, ['author', 'Sentence_Count', 'AvgSentenceLength']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b91c31",
   "metadata": {},
   "source": [
    "## Character Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(token_list):\n",
    "    \"\"\"Reconstruct text string from tokens, inserting spaces appropriately.\"\"\"\n",
    "    text = \"\"\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            # Attach punctuation directly to the text (no preceding space)\n",
    "            text += token\n",
    "        else:\n",
    "            # If not the first token, add a space before adding the word\n",
    "            if text:\n",
    "                text += \" \"\n",
    "            text += token\n",
    "    return text\n",
    "\n",
    "# List to collect n-gram frequency dictionaries for each text\n",
    "char_ngram_features = []\n",
    "\n",
    "for token_list in texts['Tokens']:\n",
    "    text = reconstruct_text(token_list)\n",
    "    # Initialize a dict for this text's n-grams\n",
    "    ngram_counts = {}\n",
    "    # Character bigrams (2-grams)\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i:i+2]\n",
    "        # Use an underscore to represent spaces in feature names for clarity\n",
    "        feat_name = \"bi_\" + bigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    # Character trigrams (3-grams)\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i+3]\n",
    "        feat_name = \"tri_\" + trigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    char_ngram_features.append(ngram_counts)\n",
    "\n",
    "# Convert list of dicts to DataFrame (each n-gram becomes a column, fill missing with 0)\n",
    "char_ngram_df = pd.DataFrame(char_ngram_features).fillna(0).astype(int)\n",
    "# Merge the n-gram features into the main DataFrame\n",
    "texts = pd.concat([texts, char_ngram_df], axis=1)\n",
    "\n",
    "# Example: number of character n-gram feature columns\n",
    "print(f\"Total character n-gram features: {char_ngram_df.shape[1]}\")\n",
    "# Show a small sample of character n-gram features for the first text\n",
    "print(texts.loc[0, [col for col in texts.columns if col.startswith('bi_')][:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79497473",
   "metadata": {},
   "source": [
    "## Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67104915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Tokens column and the intermediate WordCount column from the DataFrame\n",
    "feature_columns = [col for col in texts.columns if col not in ('Tokens', 'WordCount')]\n",
    "features_df = texts[feature_columns].copy()\n",
    "\n",
    "# Show the final feature DataFrame shape and columns\n",
    "print(\"Final feature DataFrame shape:\", features_df.shape)\n",
    "print(\"Sample columns:\", list(features_df.columns[:10]))\n",
    "# Display the first few rows of the feature DataFrame\n",
    "features_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-paper-9dnSQuhV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
