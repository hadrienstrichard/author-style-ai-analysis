{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b9964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f155e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hadrienstrichard/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66619d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aa90c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Excerpt_ID</th>\n",
       "      <th>Excerpt_Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>of_which_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_1</td>\n",
       "      <td>n'est souvent déterminée que par un mot. En ce...</td>\n",
       "      <td>n'est souvent déterminée que par un mot. en ce...</td>\n",
       "      <td>[n'est, souvent, déterminée, que, par, un, mot...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_2</td>\n",
       "      <td>Le reste ne me regarde point. J'ai dit de qui ...</td>\n",
       "      <td>le reste ne me regarde point. j'ai dit de qui ...</td>\n",
       "      <td>[le, reste, ne, me, regarde, point, ., j'ai, d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_3</td>\n",
       "      <td>Les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>[les, sylphes, ,, tout, étourdis, du, bruit, d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_4</td>\n",
       "      <td>À peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>à peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>[à, peine, mes, yeux, sont, fermés, ,, à, pein...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_5</td>\n",
       "      <td>C'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>c'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>[c'est, en, vain, que, le, jour, s'éteindrait,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                              Title  \\\n",
       "0  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "1  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "2  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "3  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "4  Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "\n",
       "                                      URL Excerpt_ID  \\\n",
       "0  https://www.gutenberg.org/ebooks/18083    18083_1   \n",
       "1  https://www.gutenberg.org/ebooks/18083    18083_2   \n",
       "2  https://www.gutenberg.org/ebooks/18083    18083_3   \n",
       "3  https://www.gutenberg.org/ebooks/18083    18083_4   \n",
       "4  https://www.gutenberg.org/ebooks/18083    18083_5   \n",
       "\n",
       "                                        Excerpt_Text  \\\n",
       "0  n'est souvent déterminée que par un mot. En ce...   \n",
       "1  Le reste ne me regarde point. J'ai dit de qui ...   \n",
       "2  Les sylphes, tout étourdis du bruit de la veil...   \n",
       "3  À peine mes yeux sont fermés, à peine cesse la...   \n",
       "4  C'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                        Cleaned_Text  \\\n",
       "0  n'est souvent déterminée que par un mot. en ce...   \n",
       "1  le reste ne me regarde point. j'ai dit de qui ...   \n",
       "2  les sylphes, tout étourdis du bruit de la veil...   \n",
       "3  à peine mes yeux sont fermés, à peine cesse la...   \n",
       "4  c'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                              Tokens Tokenized_Text  \\\n",
       "0  [n'est, souvent, déterminée, que, par, un, mot...           None   \n",
       "1  [le, reste, ne, me, regarde, point, ., j'ai, d...           None   \n",
       "2  [les, sylphes, ,, tout, étourdis, du, bruit, d...           None   \n",
       "3  [à, peine, mes, yeux, sont, fermés, ,, à, pein...           None   \n",
       "4  [c'est, en, vain, que, le, jour, s'éteindrait,...           None   \n",
       "\n",
       "  of_which_generated  \n",
       "0               None  \n",
       "1               None  \n",
       "2               None  \n",
       "3               None  \n",
       "4               None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_parquet('../Data/excerpts_df.parquet')\n",
    "\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd8481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14680 entries, 0 to 14679\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Author              14680 non-null  object\n",
      " 1   Title               14680 non-null  object\n",
      " 2   URL                 14680 non-null  object\n",
      " 3   Excerpt_ID          14680 non-null  object\n",
      " 4   Excerpt_Text        14680 non-null  object\n",
      " 5   Cleaned_Text        14680 non-null  object\n",
      " 6   Tokens              14680 non-null  object\n",
      " 7   Tokenized_Text      55 non-null     object\n",
      " 8   of_which_generated  55 non-null     object\n",
      "dtypes: object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5526b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of French function words: 157\n",
      "Sample function words: ['s', 'du', 'ayante', 'ayant', 'eurent', 'étés', 'aies', 'serez', 'l', 'avez']\n"
     ]
    }
   ],
   "source": [
    "# Load French function words (common stopwords in French) as our function word list.\n",
    "french_function_words = set(stopwords.words('french'))\n",
    "print(f\"Number of French function words: {len(french_function_words)}\")\n",
    "# Optionally, inspect a few function words\n",
    "print(\"Sample function words:\", list(french_function_words)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of punctuation marks we will consider for certain features.\n",
    "punct_set = {'.', ',', ';', '!', '?', ':'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943ab7a",
   "metadata": {},
   "source": [
    "## Function word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3eb410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author       Charles Nodier\n",
      "fw_s                    0.0\n",
      "fw_du              0.012085\n",
      "fw_ayante               0.0\n",
      "fw_ayant                0.0\n",
      "fw_eurent               0.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Compute total number of words (excluding punctuation) for each text, to use in normalization\n",
    "texts['WordCount'] = texts['Tokens'].apply(lambda tokens: sum(1 for t in tokens if t not in punct_set))\n",
    "\n",
    "# Add one feature column for each function word in the list\n",
    "for fw in french_function_words:\n",
    "    col_name = f\"fw_{fw}\"\n",
    "    # Normalized frequency: count of fw divided by number of words (avoid division by zero)\n",
    "    texts[col_name] = texts.apply(\n",
    "        lambda row: row['Tokens'].count(fw) / row['WordCount'] if row['WordCount'] > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Example: check a few function word feature columns for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"fw_{w}\" for w in list(french_function_words)[:5]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe22b",
   "metadata": {},
   "source": [
    "## Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80a57413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Author  POS_NOUN  POS_VERB   POS_ADJ\n",
      "0  Charles Nodier  0.195108  0.113952  0.081156\n",
      "1  Charles Nodier  0.230860  0.114252  0.068316\n",
      "2  Charles Nodier  0.246499  0.127731  0.075630\n",
      "3  Charles Nodier  0.245342  0.123602  0.073292\n",
      "4  Charles Nodier  0.230182  0.118614  0.076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_8768/2059325842.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts['POS_NOUN'] = pos_noun_freq\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_8768/2059325842.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts['POS_VERB'] = pos_verb_freq\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_8768/2059325842.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts['POS_ADJ']  = pos_adj_freq\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to collect POS frequency features for each text\n",
    "pos_noun_freq = []\n",
    "pos_verb_freq = []\n",
    "pos_adj_freq = []\n",
    "\n",
    "for tokens in texts['Tokens']:\n",
    "    # Join tokens back into a text string for POS tagging\n",
    "    doc = nlp_fr(\" \".join(tokens))\n",
    "    # Count total number of word tokens (non-punctuation) in this text\n",
    "    total_words = sum(1 for token in doc if token.pos_ != \"PUNCT\")\n",
    "    # Count specific POS categories\n",
    "    noun_count = sum(1 for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"])  # treat proper nouns as nouns\n",
    "    verb_count = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    adj_count  = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
    "    # Compute relative frequencies (avoid division by zero)\n",
    "    if total_words > 0:\n",
    "        pos_noun_freq.append(noun_count / total_words)\n",
    "        pos_verb_freq.append(verb_count / total_words)\n",
    "        pos_adj_freq.append(adj_count / total_words)\n",
    "    else:\n",
    "        pos_noun_freq.append(0.0)\n",
    "        pos_verb_freq.append(0.0)\n",
    "        pos_adj_freq.append(0.0)\n",
    "\n",
    "# Add POS distribution features to the DataFrame\n",
    "texts['POS_NOUN'] = pos_noun_freq\n",
    "texts['POS_VERB'] = pos_verb_freq\n",
    "texts['POS_ADJ']  = pos_adj_freq\n",
    "\n",
    "# Example: view POS distribution for first few texts\n",
    "print(texts.loc[:4, ['Author', 'POS_NOUN', 'POS_VERB', 'POS_ADJ']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928292d0",
   "metadata": {},
   "source": [
    "## Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ecb6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['author'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m texts = pd.concat([texts, lexical_df], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Example: show lexical richness metrics for a text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtexts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauthor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWordCount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTTR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHapax_Ratio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAvgWordLen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1184\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1187\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1368\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[32m   1367\u001b[39m     tup = \u001b[38;5;28mself\u001b[39m._expand_ellipsis(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[32m   1371\u001b[39m tup = \u001b[38;5;28mself\u001b[39m._validate_tuple_indexer(tup)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1089\u001b[39m, in \u001b[36m_LocationIndexer._getitem_lowerdim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1087\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[32m   1088\u001b[39m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[33m\"\u001b[39m\u001b[33mnot applicable\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1359\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1362\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1363\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1555\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1556\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['author'] not in index\""
     ]
    }
   ],
   "source": [
    "def compute_lexical_metrics(token_list):\n",
    "    # Remove punctuation and non-word tokens from the list\n",
    "    words = []\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            continue  # skip punctuation tokens\n",
    "        # Remove any punctuation attached to the token (e.g., apostrophes or hyphens) for word length calculation\n",
    "        cleaned_token = re.sub(r'[\\W_]+', '', token, flags=re.UNICODE)  # keep only alphanumeric characters\n",
    "        if cleaned_token == \"\" or cleaned_token.isdigit():\n",
    "            continue  # skip if token is empty or purely numeric after cleaning\n",
    "        words.append(cleaned_token)\n",
    "    if len(words) == 0:\n",
    "        # If no valid word tokens, return zeros\n",
    "        return pd.Series({\"TTR\": 0.0, \"Hapax_Ratio\": 0.0, \"AvgWordLen\": 0.0})\n",
    "    total_words = len(words)\n",
    "    unique_words = set(words)\n",
    "    # Type-Token Ratio\n",
    "    ttr = len(unique_words) / total_words\n",
    "    # Hapax Legomena Ratio\n",
    "    word_freqs = Counter(words)\n",
    "    hapax_count = sum(1 for count in word_freqs.values() if count == 1)\n",
    "    hapax_ratio = hapax_count / total_words\n",
    "    # Average word length (in characters)\n",
    "    total_chars = sum(len(w) for w in words)\n",
    "    avg_word_len = total_chars / total_words\n",
    "    return pd.Series({\"TTR\": ttr, \"Hapax_Ratio\": hapax_ratio, \"AvgWordLen\": avg_word_len})\n",
    "\n",
    "# Apply the lexical metrics function to each text\n",
    "lexical_df = texts['Tokens'].apply(compute_lexical_metrics)\n",
    "# Merge the resulting metrics columns into the main DataFrame\n",
    "texts = pd.concat([texts, lexical_df], axis=1)\n",
    "\n",
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['author', 'WordCount', 'TTR', 'Hapax_Ratio', 'AvgWordLen']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36637095",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of punctuation symbols to descriptive column names\n",
    "punct_marks = {'.': 'Period', ',': 'Comma', ';': 'Semicolon', '?': 'QuestionMark', '!': 'ExclamationMark', ':': 'Colon'}\n",
    "\n",
    "for symbol, name in punct_marks.items():\n",
    "    texts[f\"{name}_Count\"] = texts['Tokens'].apply(lambda tokens: tokens.count(symbol))\n",
    "\n",
    "# Example: show punctuation counts for the first text\n",
    "print(texts.loc[0, ['author'] + [f\"{name}_Count\" for name in punct_marks.values()]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148761dc",
   "metadata": {},
   "source": [
    "## Sentence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50330474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_metrics(token_list):\n",
    "    # Count sentence-ending punctuation marks\n",
    "    sentence_enders = {'.', '!', '?'}\n",
    "    sent_count = sum(token_list.count(sym) for sym in sentence_enders)\n",
    "    if sent_count == 0:\n",
    "        # If no explicit end markers but there are words, assume at least one sentence\n",
    "        sent_count = 1 if any(t not in punct_set for t in token_list) else 0\n",
    "    # Count words (exclude all punctuation tokens)\n",
    "    word_count = sum(1 for t in token_list if t not in punct_set)\n",
    "    avg_sent_len = word_count / sent_count if sent_count > 0 else 0.0\n",
    "    return pd.Series({\"Sentence_Count\": sent_count, \"AvgSentenceLength\": avg_sent_len})\n",
    "\n",
    "sentence_df = texts['Tokens'].apply(compute_sentence_metrics)\n",
    "texts = pd.concat([texts, sentence_df], axis=1)\n",
    "\n",
    "# Example: show sentence metrics for first few texts\n",
    "print(texts.loc[:4, ['author', 'Sentence_Count', 'AvgSentenceLength']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b91c31",
   "metadata": {},
   "source": [
    "## Character Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(token_list):\n",
    "    \"\"\"Reconstruct text string from tokens, inserting spaces appropriately.\"\"\"\n",
    "    text = \"\"\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            # Attach punctuation directly to the text (no preceding space)\n",
    "            text += token\n",
    "        else:\n",
    "            # If not the first token, add a space before adding the word\n",
    "            if text:\n",
    "                text += \" \"\n",
    "            text += token\n",
    "    return text\n",
    "\n",
    "# List to collect n-gram frequency dictionaries for each text\n",
    "char_ngram_features = []\n",
    "\n",
    "for token_list in texts['Tokens']:\n",
    "    text = reconstruct_text(token_list)\n",
    "    # Initialize a dict for this text's n-grams\n",
    "    ngram_counts = {}\n",
    "    # Character bigrams (2-grams)\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i:i+2]\n",
    "        # Use an underscore to represent spaces in feature names for clarity\n",
    "        feat_name = \"bi_\" + bigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    # Character trigrams (3-grams)\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i+3]\n",
    "        feat_name = \"tri_\" + trigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    char_ngram_features.append(ngram_counts)\n",
    "\n",
    "# Convert list of dicts to DataFrame (each n-gram becomes a column, fill missing with 0)\n",
    "char_ngram_df = pd.DataFrame(char_ngram_features).fillna(0).astype(int)\n",
    "# Merge the n-gram features into the main DataFrame\n",
    "texts = pd.concat([texts, char_ngram_df], axis=1)\n",
    "\n",
    "# Example: number of character n-gram feature columns\n",
    "print(f\"Total character n-gram features: {char_ngram_df.shape[1]}\")\n",
    "# Show a small sample of character n-gram features for the first text\n",
    "print(texts.loc[0, [col for col in texts.columns if col.startswith('bi_')][:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79497473",
   "metadata": {},
   "source": [
    "## Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67104915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Tokens column and the intermediate WordCount column from the DataFrame\n",
    "feature_columns = [col for col in texts.columns if col not in ('Tokens', 'WordCount')]\n",
    "features_df = texts[feature_columns].copy()\n",
    "\n",
    "# Show the final feature DataFrame shape and columns\n",
    "print(\"Final feature DataFrame shape:\", features_df.shape)\n",
    "print(\"Sample columns:\", list(features_df.columns[:10]))\n",
    "# Display the first few rows of the feature DataFrame\n",
    "features_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-paper-9dnSQuhV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
