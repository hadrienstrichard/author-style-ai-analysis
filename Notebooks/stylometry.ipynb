{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b9964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Uncomment the following lines if you want to use the freestylo package\n",
    "'''\n",
    "from freestylo import TextObject\n",
    "from freestylo import TextPreprocessor\n",
    "from freestylo.AlliterationAnnotation import AlliterationAnnotation\n",
    "from freestylo.EpiphoraAnnotation import EpiphoraAnnotation\n",
    "from freestylo.PolysyndetonAnnotation import PolysyndetonAnnotation\n",
    "'''\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb209f",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f155e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hadrienstrichard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66619d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35aa90c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Excerpt_ID</th>\n",
       "      <th>Excerpt_Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_1</td>\n",
       "      <td>n'est souvent déterminée que par un mot. En ce...</td>\n",
       "      <td>n'est souvent déterminée que par un mot. en ce...</td>\n",
       "      <td>[n'est, souvent, déterminée, que, par, un, mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_2</td>\n",
       "      <td>Le reste ne me regarde point. J'ai dit de qui ...</td>\n",
       "      <td>le reste ne me regarde point. j'ai dit de qui ...</td>\n",
       "      <td>[le, reste, ne, me, regarde, point, ., j'ai, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_3</td>\n",
       "      <td>Les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>les sylphes, tout étourdis du bruit de la veil...</td>\n",
       "      <td>[les, sylphes, ,, tout, étourdis, du, bruit, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_4</td>\n",
       "      <td>À peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>à peine mes yeux sont fermés, à peine cesse la...</td>\n",
       "      <td>[à, peine, mes, yeux, sont, fermés, ,, à, pein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Nodier</td>\n",
       "      <td>Smarra ou les démons de la nuit: Songes romant...</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/18083</td>\n",
       "      <td>18083_5</td>\n",
       "      <td>C'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>c'est en vain que le jour s'éteindrait, tant q...</td>\n",
       "      <td>[c'est, en, vain, que, le, jour, s'éteindrait,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Author                                              Title  \\\n",
       "index                                                                      \n",
       "0      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "1      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "2      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "3      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "4      Charles Nodier  Smarra ou les démons de la nuit: Songes romant...   \n",
       "\n",
       "                                          URL Excerpt_ID  \\\n",
       "index                                                      \n",
       "0      https://www.gutenberg.org/ebooks/18083    18083_1   \n",
       "1      https://www.gutenberg.org/ebooks/18083    18083_2   \n",
       "2      https://www.gutenberg.org/ebooks/18083    18083_3   \n",
       "3      https://www.gutenberg.org/ebooks/18083    18083_4   \n",
       "4      https://www.gutenberg.org/ebooks/18083    18083_5   \n",
       "\n",
       "                                            Excerpt_Text  \\\n",
       "index                                                      \n",
       "0      n'est souvent déterminée que par un mot. En ce...   \n",
       "1      Le reste ne me regarde point. J'ai dit de qui ...   \n",
       "2      Les sylphes, tout étourdis du bruit de la veil...   \n",
       "3      À peine mes yeux sont fermés, à peine cesse la...   \n",
       "4      C'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                            Cleaned_Text  \\\n",
       "index                                                      \n",
       "0      n'est souvent déterminée que par un mot. en ce...   \n",
       "1      le reste ne me regarde point. j'ai dit de qui ...   \n",
       "2      les sylphes, tout étourdis du bruit de la veil...   \n",
       "3      à peine mes yeux sont fermés, à peine cesse la...   \n",
       "4      c'est en vain que le jour s'éteindrait, tant q...   \n",
       "\n",
       "                                                  Tokens  \n",
       "index                                                     \n",
       "0      [n'est, souvent, déterminée, que, par, un, mot...  \n",
       "1      [le, reste, ne, me, regarde, point, ., j'ai, d...  \n",
       "2      [les, sylphes, ,, tout, étourdis, du, bruit, d...  \n",
       "3      [à, peine, mes, yeux, sont, fermés, ,, à, pein...  \n",
       "4      [c'est, en, vain, que, le, jour, s'éteindrait,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_parquet('../Data/excerpts_processed.parquet')\n",
    "\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd8481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Author        14722 non-null  object\n",
      " 1   Title         14722 non-null  object\n",
      " 2   URL           14722 non-null  object\n",
      " 3   Excerpt_ID    14722 non-null  object\n",
      " 4   Excerpt_Text  14722 non-null  object\n",
      " 5   Cleaned_Text  14722 non-null  object\n",
      " 6   Tokens        14722 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 920.1+ KB\n"
     ]
    }
   ],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5526b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of French function words: 157\n",
      "Sample function words: ['nos', 'serions', 'ont', 'eusse', 'sois', 'ou', 'étées', 'fusses', 'mes', 'qu']\n"
     ]
    }
   ],
   "source": [
    "# Load French function words (common stopwords in French) as our function word list.\n",
    "french_function_words = set(stopwords.words('french'))\n",
    "print(f\"Number of French function words: {len(french_function_words)}\")\n",
    "# Optionally, inspect a few function words\n",
    "print(\"Sample function words:\", list(french_function_words)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of punctuation marks we will consider for certain features.\n",
    "punct_set = {'.', ',', ';', '!', '?', ':', '-', '—', '(', ')', '[', ']', '{', '}', '\"', \"'\", '«', '»'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943ab7a",
   "metadata": {},
   "source": [
    "## Function word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3eb410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author       Charles Nodier\n",
      "fw_avions               0.0\n",
      "fw_votre                0.0\n",
      "fw_étiez                0.0\n",
      "fw_sont            0.000607\n",
      "fw_auront               0.0\n",
      "Name: 0, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 165 entries, Author to fw_du\n",
      "dtypes: float64(157), int64(1), object(7)\n",
      "memory usage: 19.1+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3428490745.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[col_name] = texts.apply(\n"
     ]
    }
   ],
   "source": [
    "# Compute total number of words (excluding punctuation) for each text, to use in normalization\n",
    "texts['WordCount'] = texts['Tokens'].apply(lambda tokens: sum(1 for t in tokens if t not in punct_set))\n",
    "\n",
    "# Add one feature column for each function word in the list\n",
    "for fw in french_function_words:\n",
    "    col_name = f\"fw_{fw}\"\n",
    "    # Normalized frequency: count of fw divided by number of words (avoid division by zero)\n",
    "    texts[col_name] = texts.apply(\n",
    "        lambda row: row['Tokens'].count(fw) / row['WordCount'] if row['WordCount'] > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Example: check a few function word feature columns for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"fw_{w}\" for w in list(french_function_words)[:5]]])\n",
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f985ff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3534742479.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3534742479.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3534742479.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n",
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3534742479.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fw_article        0.112325\n",
      "fw_preposition    0.105647\n",
      "fw_pronoun        0.078931\n",
      "fw_conjunction    0.063145\n",
      "fw_auxiliary      0.001214\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/v7b7dbbn6xg3hzr5h4395gbc0000gn/T/ipykernel_1071/3534742479.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  texts[group_name] = texts.apply(\n"
     ]
    }
   ],
   "source": [
    "# Define function word groups\n",
    "func_word_groups = {\n",
    "    \"gfw_article\": {\"le\", \"la\", \"les\", \"un\", \"une\", \"du\", \"des\"},\n",
    "    \"gfw_preposition\": {\"à\", \"de\", \"en\", \"dans\", \"avec\", \"sans\", \"sous\", \"sur\", \"chez\"},\n",
    "    \"gfw_pronoun\": {\"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \"on\", \"lui\", \"leur\", \"se\", \"me\", \"te\", \"moi\", \"toi\", \"qui\", \"que\", \"quoi\"},\n",
    "    \"gfw_conjunction\": {\"et\", \"ou\", \"mais\", \"donc\", \"or\", \"car\", \"puisque\", \"lorsque\", \"parce\", \"que\", \"si\"},\n",
    "    \"gfw_auxiliary\": {\"être\", \"avoir\"}\n",
    "}\n",
    "\n",
    "# Compute per-group normalized frequencies\n",
    "for group_name, word_set in func_word_groups.items():\n",
    "    texts[group_name] = texts.apply(\n",
    "        lambda row: sum(row['Tokens'].count(w) for w in word_set) / row['WordCount']\n",
    "        if row['WordCount'] > 0 else 0.0, axis=1\n",
    "    )\n",
    "\n",
    "# Example: check the computed features for the first text\n",
    "print(texts.loc[0, ['gfw_article', 'gfw_preposition', 'gfw_pronoun', 'gfw_conjunction', 'gfw_auxiliary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e80b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14722 entries, 0 to 14754\n",
      "Columns: 170 entries, Author to fw_auxiliary\n",
      "dtypes: float64(162), int64(1), object(7)\n",
      "memory usage: 19.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe22b",
   "metadata": {},
   "source": [
    "## Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a57413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x110ce1cd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hadrienstrichard/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.11/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Define POS tags of interest\n",
    "pos_tags = ['NOUN', 'PROPN', 'VERB', 'AUX', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'CCONJ', 'SCONJ', 'INTJ']\n",
    "\n",
    "# Initialize POS frequency containers\n",
    "pos_freqs = defaultdict(list)\n",
    "\n",
    "for tokens in texts['Tokens']:\n",
    "    doc = nlp_fr(\" \".join(tokens))\n",
    "    total_words = sum(1 for token in doc if token.pos_ != \"PUNCT\")\n",
    "    \n",
    "    pos_counts = Counter(token.pos_ for token in doc if token.pos_ != \"PUNCT\")\n",
    "    pos_counts['NOUN'] += pos_counts.get('PROPN', 0)  # Merge PROPN into NOUN\n",
    "    for pos in pos_tags:\n",
    "        freq = pos_counts.get(pos, 0) / total_words if total_words > 0 else 0.0\n",
    "        pos_freqs[pos].append(freq)\n",
    "\n",
    "# Assign POS frequency features to the DataFrame\n",
    "for pos in pos_tags:\n",
    "    col = 'POS_' + ('NOUN' if pos == 'PROPN' else pos)\n",
    "    if col not in texts.columns:\n",
    "        texts[col] = pos_freqs[pos]\n",
    "        \n",
    "# Example: view the POS distribution for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"POS_{w}\" for w in list(pos_tags)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928292d0",
   "metadata": {},
   "source": [
    "## Lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ecb6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author         Charles Nodier\n",
      "WordCount                1647\n",
      "TTR                  0.448087\n",
      "Hapax_Ratio          0.327869\n",
      "AvgWordLen           4.870674\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def compute_lexical_metrics(token_list):\n",
    "    # Remove punctuation and non-word tokens from the list\n",
    "    words = []\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            continue  # skip punctuation tokens\n",
    "        # Remove any punctuation attached to the token (e.g., apostrophes or hyphens) for word length calculation\n",
    "        cleaned_token = re.sub(r'[\\W_]+', '', token, flags=re.UNICODE)  # keep only alphanumeric characters\n",
    "        if cleaned_token == \"\" or cleaned_token.isdigit():\n",
    "            continue  # skip if token is empty or purely numeric after cleaning\n",
    "        words.append(cleaned_token)\n",
    "    if len(words) == 0:\n",
    "        # If no valid word tokens, return zeros\n",
    "        return pd.Series({\"lex_TTR\": 0.0, \"lex_Hapax_Ratio\": 0.0, \"lex_AvgWordLen\": 0.0})\n",
    "    total_words = len(words)\n",
    "    unique_words = set(words)\n",
    "    # Type-Token Ratio\n",
    "    ttr = len(unique_words) / total_words\n",
    "    # Hapax Legomena Ratio\n",
    "    word_freqs = Counter(words)\n",
    "    hapax_count = sum(1 for count in word_freqs.values() if count == 1)\n",
    "    hapax_ratio = hapax_count / total_words\n",
    "    # Average word length (in characters)\n",
    "    total_chars = sum(len(w) for w in words)\n",
    "    avg_word_len = total_chars / total_words\n",
    "    return pd.Series({\"lex_TTR\": ttr, \"lex_Hapax_Ratio\": hapax_ratio, \"lex_AvgWordLen\": avg_word_len})\n",
    "\n",
    "# Apply the lexical metrics function to each text\n",
    "lexical_df = texts['Tokens'].apply(compute_lexical_metrics)\n",
    "# Merge the resulting metrics columns into the main DataFrame\n",
    "texts = pd.concat([texts, lexical_df], axis=1)\n",
    "\n",
    "# Example: show lexical richness metrics for a text\n",
    "print(texts.loc[0, ['Author', 'WordCount', 'lex_TTR', 'lex_Hapax_Ratio', 'lex_AvgWordLen']])\n",
    "print(texts.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36637095",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabd3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author                   Charles Nodier\n",
      "Period_Count                         51\n",
      "Comma_Count                         116\n",
      "Semicolon_Count                       6\n",
      "QuestionMark_Count                    0\n",
      "ExclamationMark_Count                 2\n",
      "Colon_Count                           3\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping of punctuation symbols to descriptive column names\n",
    "punct_marks = {'.': 'Period', ',': 'Comma', ';': 'Semicolon', '?': 'QuestionMark', '!': 'ExclamationMark', ':': 'Colon'}\n",
    "\n",
    "for symbol, name in punct_marks.items():\n",
    "    texts[f\"{name}_Count\"] = texts['Tokens'].apply(lambda tokens: tokens.count(symbol))\n",
    "\n",
    "# Example: show punctuation counts for the first text\n",
    "print(texts.loc[0, ['Author'] + [f\"pun_{name}_Count\" for name in punct_marks.values()]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148761dc",
   "metadata": {},
   "source": [
    "## Sentence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac16862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_metrics_with_complexity(token_list):\n",
    "    # Count sentence-ending punctuation marks\n",
    "    sentence_enders = {'.', '!', '?'}\n",
    "    \n",
    "    # Count words (exclude all punctuation tokens)\n",
    "    sent_count = sum(token_list.count(sym) for sym in sentence_enders)\n",
    "    sent_count = max(sent_count, 1 if any(t not in punct_set for t in token_list) else 0)\n",
    "    word_count = sum(1 for t in token_list if t not in punct_set)\n",
    "    avg_sent_len = word_count / sent_count if sent_count > 0 else 0.0\n",
    "\n",
    "    # Re-parse to extract dependency info\n",
    "    doc = nlp_fr(\" \".join(token_list))\n",
    "    sub_clause_deps = {\"ccomp\", \"xcomp\", \"advcl\", \"relcl\", \"acl\", \"mark\"}\n",
    "    sub_clause_count = sum(1 for token in doc if token.dep_ in sub_clause_deps)\n",
    "\n",
    "    complexity_score = sub_clause_count / sent_count if sent_count > 0 else 0.0\n",
    "    return pd.Series({\n",
    "        \"Sentence_Count\": sent_count,\n",
    "        \"Sentence_AvgLength\": avg_sent_len,\n",
    "        \"Sentence_Complexity\": complexity_score\n",
    "    })\n",
    "\n",
    "sentence_df = texts['Tokens'].apply(compute_sentence_metrics_with_complexity)\n",
    "texts = pd.concat([texts, sentence_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeb671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tense/mood patterns to track\n",
    "tense_patterns = {\n",
    "    \"tense_pres_ind\": (\"Tense=Pres\", \"Mood=Ind\"),\n",
    "    \"tense_past_ind\": (\"Tense=Past\", \"Mood=Ind\"),\n",
    "    \"tense_imp_ind\": (\"Tense=Imp\", \"Mood=Ind\"),\n",
    "    \"tense_cond\": (\"Mood=Cnd\",),\n",
    "    \"tense_subj\": (\"Mood=Sub\",),\n",
    "    \"tense_inf\": (\"VerbForm=Inf\",)\n",
    "}\n",
    "\n",
    "# Initialize dict of lists\n",
    "tense_counts = {k: [] for k in tense_patterns.keys()}\n",
    "\n",
    "for tokens in texts['Tokens']:\n",
    "    doc = nlp_fr(\" \".join(tokens))\n",
    "    verb_tokens = [t for t in doc if t.pos_ in {\"VERB\", \"AUX\"}]\n",
    "    total_verbs = len(verb_tokens)\n",
    "    \n",
    "    for label, conditions in tense_patterns.items():\n",
    "        count = sum(\n",
    "            all(cond in t.morph.to_json() for cond in conditions)\n",
    "            for t in verb_tokens\n",
    "        )\n",
    "        tense_counts[label].append(count / total_verbs if total_verbs > 0 else 0.0)\n",
    "\n",
    "# Add to DataFrame\n",
    "for label, vals in tense_counts.items():\n",
    "    texts[label] = vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b91c31",
   "metadata": {},
   "source": [
    "## Character Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total character n-gram features: 28205\n",
      "bi_n'     20.0\n",
      "bi_'e     37.0\n",
      "bi_es    206.0\n",
      "bi_st     32.0\n",
      "bi_t_    201.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Commented out as it added too much features for too little gain\n",
    "\n",
    "'''\n",
    "\n",
    "def reconstruct_text(token_list):\n",
    "    \"\"\"Reconstruct text string from tokens, inserting spaces appropriately.\"\"\"\n",
    "    text = \"\"\n",
    "    for token in token_list:\n",
    "        if token in punct_set:\n",
    "            # Attach punctuation directly to the text (no preceding space)\n",
    "            text += token\n",
    "        else:\n",
    "            # If not the first token, add a space before adding the word\n",
    "            if text:\n",
    "                text += \" \"\n",
    "            text += token\n",
    "    return text\n",
    "\n",
    "# List to collect n-gram frequency dictionaries for each text\n",
    "char_ngram_features = []\n",
    "\n",
    "for token_list in texts['Tokens']:\n",
    "    text = reconstruct_text(token_list)\n",
    "    # Initialize a dict for this text's n-grams\n",
    "    ngram_counts = {}\n",
    "    # Character bigrams (2-grams)\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i:i+2]\n",
    "        # Use an underscore to represent spaces in feature names for clarity\n",
    "        feat_name = \"bi_\" + bigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    # Character trigrams (3-grams)\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i+3]\n",
    "        feat_name = \"tri_\" + trigram.replace(\" \", \"_\")\n",
    "        ngram_counts[feat_name] = ngram_counts.get(feat_name, 0) + 1\n",
    "    char_ngram_features.append(ngram_counts)\n",
    "\n",
    "# Convert list of dicts to DataFrame (each n-gram becomes a column, fill missing with 0)\n",
    "char_ngram_df = pd.DataFrame(char_ngram_features).fillna(0).astype(int)\n",
    "# Merge the n-gram features into the main DataFrame\n",
    "texts = pd.concat([texts, char_ngram_df], axis=1)\n",
    "\n",
    "# Example: number of character n-gram feature columns\n",
    "print(f\"Total character n-gram features: {char_ngram_df.shape[1]}\")\n",
    "# Show a small sample of character n-gram features for the first text\n",
    "print(texts.loc[0, [col for col in texts.columns if col.startswith('bi_')][:5]])\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2566b",
   "metadata": {},
   "source": [
    "## Figure of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44caa73c",
   "metadata": {},
   "source": [
    "Avec freestylo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out as it made the computer freeze\n",
    "\n",
    "'''\n",
    "\n",
    "FIGURES = [AlliterationAnnotation,\n",
    "           EpiphoraAnnotation, PolysyndetonAnnotation]\n",
    "\n",
    "def extract_stylistic_features(text, lang='fr', nlp=None):\n",
    "    tobj = TextObject.TextObject(text=text, language=lang)\n",
    "    tp = TextPreprocessor.TextPreprocessor(language=lang)\n",
    "    tp.nlp = nlp\n",
    "    tp.process_text(tobj)\n",
    "\n",
    "    features, examples = {}, {}\n",
    "    for AnnotClass in FIGURES:\n",
    "        annot = AnnotClass(text=tobj)\n",
    "        annot.find_candidates()\n",
    "        if hasattr(annot, 'load_model'):\n",
    "            annot.load_model()\n",
    "        if hasattr(annot, 'score_candidates'):\n",
    "            annot.score_candidates()\n",
    "\n",
    "        instances = annot.candidates \n",
    "        features[f\"fig_{AnnotClass.__name__}\"] = len(instances)\n",
    "        examples[AnnotClass.__name__] = instances\n",
    "    return features, examples\n",
    "\n",
    "\n",
    "def apply_stylistic_features(df, column='Cleaned_Text', nlp=None):\n",
    "    feats_list, ex_list = [], []\n",
    "\n",
    "    for txt in tqdm(df[column].astype(str), desc=\"Extracting stylistic features\"):\n",
    "        feats, ex = extract_stylistic_features(txt, nlp=nlp)\n",
    "        feats_list.append(feats)\n",
    "        ex_list.append(ex)\n",
    "\n",
    "    feats_df = pd.DataFrame(feats_list).fillna(0).astype(int)\n",
    "    return pd.concat([df.reset_index(drop=True), feats_df], axis=1), ex_list\n",
    "    \n",
    "texts, figure_examples = apply_stylistic_features(texts, nlp=nlp_fr)\n",
    "\n",
    "texts['fig_AlliterationAnnotation', 'fig_EpiphoraAnnotation', 'fig_PolysyndetonAnnotation'].describe()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3f82e",
   "metadata": {},
   "source": [
    "LLM labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77d534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48a5f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_figures(phrase: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "Tu es un expert en analyse littéraire. Analyse la phrase suivante et indique quelles figures de style elle contient, en te basant sur cette liste. Pour chaque figure, réponds par vrai ou faux. Voici les figures à analyser, avec leur définition et un exemple pour chacune :\n",
    "\n",
    "1. **Comparaison** : Mise en relation d’un comparé et d’un comparant avec un outil (comme, tel, semblable à, etc.).\n",
    "   Exemple : « Cet homme est bête comme ses pieds »\n",
    "2. **Métaphore** : Comparaison sans outil de comparaison.\n",
    "   Exemple : « Ses cheveux de miel. »\n",
    "3. **Personnification** : Attribution de caractéristiques humaines à un objet ou un animal.\n",
    "   Exemple : « Le stylo saute de la table »\n",
    "4. **Hyperbole** : Exagération dans le but de frapper l’imagination.\n",
    "   Exemple : « Je meurs de faim »\n",
    "5. **Litote** : Dire moins pour faire entendre plus.\n",
    "   Exemple : « Je ne te hais point »\n",
    "6. **Antithèse** : Opposition très forte entre deux idées.\n",
    "   Exemple : « Je vis, je meurs »\n",
    "7. **Oxymore** : Réunion de deux termes opposés dans un même groupe de mots.\n",
    "   Exemple : « Une obscure clarté »\n",
    "8. **Répétition** : Reprise d’un même mot plusieurs fois.\n",
    "   Exemple : « La guerre, la guerre, la guerre ! »\n",
    "9. **Énumération** : Accumulation de mots de même nature grammaticale.\n",
    "   Exemple : « Il mange des pommes, des poires, des bananes et des kiwis. »\n",
    "10. **Euphémisme** : Expression atténuée d’une idée jugée brutale.\n",
    "   Exemple : « Il nous a quittés » (pour \"il est mort\")\n",
    "\n",
    "Phrase à analyser :\n",
    "« {phrase} »\n",
    "\n",
    "Réponds uniquement avec un dictionnaire JSON contenant ces clés : \"Comparaison\", \"Métaphore\", \"Personnification\", \"Hyperbole\", \"Litote\", \"Antithèse\", \"Oxymore\", \"Répétition\", \"Énumération\", \"Euphémisme\", et pour chaque clé, la valeur `true` ou `false` (sans guillemets).\n",
    "\n",
    "Ne donne pas d’explication.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    # Extraction de la sortie générée\n",
    "    generated = response.choices[0].message.content\n",
    "\n",
    "    # Évaluation sécurisée du JSON renvoyé (sans `eval`)\n",
    "    import json\n",
    "    try:\n",
    "        result = json.loads(generated)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Parsing error:\", generated)\n",
    "        result = {}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "011d07eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Comparaison': False,\n",
       " 'Métaphore': False,\n",
       " 'Personnification': False,\n",
       " 'Hyperbole': False,\n",
       " 'Litote': True,\n",
       " 'Antithèse': False,\n",
       " 'Oxymore': False,\n",
       " 'Répétition': False,\n",
       " 'Énumération': False,\n",
       " 'Euphémisme': False}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_figures('Je ne te hais point')  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608402ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Figure'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(fig_df))\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Normalize figure labels\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m fig_df[\u001b[33m\"\u001b[39m\u001b[33mFigure\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfig_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFigure\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.str.strip().str.capitalize()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     11\u001b[39m predicted_labels = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/research-paper-9dnSQuhV-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Figure'"
     ]
    }
   ],
   "source": [
    "# Test the LLM labeling function\n",
    "\n",
    "# Load the test dataset\n",
    "fig_df = pd.read_csv(\n",
    "    \"../Data/figures.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "# Normalize figure labels\n",
    "fig_df[\"Figure\"] = fig_df[\"Figure\"].str.strip().str.capitalize()\n",
    "\n",
    "# Store results\n",
    "predicted_labels = []\n",
    "correct_flags = []\n",
    "\n",
    "# Loop through each sentence and evaluate\n",
    "for i, row in tqdm(fig_df.iterrows(), total=len(fig_df), desc=\"Running LLM predictions\"):\n",
    "    sentence = row[\"Texte\"]\n",
    "    true_label = row[\"Figure\"]\n",
    "\n",
    "    result = detect_figures(sentence)\n",
    "\n",
    "    # Store full prediction dictionary (optional)\n",
    "    predicted_labels.append(result)\n",
    "\n",
    "    # Check if the true label is among the ones flagged as True\n",
    "    detected_true = result.get(true_label, False)\n",
    "    correct_flags.append(detected_true)\n",
    "\n",
    "# Add evaluation columns\n",
    "fig_df[\"LLM_Output\"] = predicted_labels\n",
    "fig_df[\"Correct\"] = correct_flags\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = fig_df[\"Correct\"].mean()\n",
    "print(f\"\\nAccuracy (relaxed): {accuracy:.2%}\")\n",
    "\n",
    "# Show errors (where the true label was not among the detected ones)\n",
    "errors_df = fig_df[~fig_df[\"Correct\"]]\n",
    "print(f\"\\nTotal errors: {len(errors_df)} / {len(fig_df)}\")\n",
    "\n",
    "# Display mismatches\n",
    "for idx, row in errors_df.iterrows():\n",
    "    print(\"\\n---\")\n",
    "    print(f\"Sentence      : {row['Texte']}\")\n",
    "    print(f\"True Label    : {row['Figure']}\")\n",
    "    print(f\"LLM Prediction: {[k for k, v in row['LLM_Output'].items() if v]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79497473",
   "metadata": {},
   "source": [
    "## Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b51cfc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14755 entries, 0 to 12596\n",
      "Columns: 28396 entries, Author to tri_iht\n",
      "dtypes: float64(28389), object(7)\n",
      "memory usage: 3.1+ GB\n"
     ]
    }
   ],
   "source": [
    "texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "977a4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (14755, 28396)\n",
      "float64    28389\n",
      "object         7\n",
      "Name: count, dtype: int64\n",
      "Missing values total: 937068\n",
      "0.22365299898339544 % of total values are missing\n"
     ]
    }
   ],
   "source": [
    "# Taille du dataset\n",
    "print(\"Shape:\", texts.shape)\n",
    "\n",
    "# Types de données\n",
    "print(texts.dtypes.value_counts())\n",
    "\n",
    "# Nombre total de valeurs manquantes\n",
    "print(\"Missing values total:\", texts.isna().sum().sum())\n",
    "\n",
    "print(texts.isna().sum().sum() / (texts.shape[0] * texts.shape[1]) * 100, \"% of total values are missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0e84805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape after column filtering: (14755, 28390)\n",
      "Total remaining NaNs: 936870\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Ne garder que les features + la colonne \"Author\"\n",
    "# On supprime tout sauf \"Author\" et les colonnes de type numérique (features)\n",
    "cols_to_keep = ['Author'] + [col for col in texts.columns if col not in ['Title', 'URL', 'Excerpt_ID', 'Excerpt_Text', 'Cleaned_Text', 'Tokens'] and col != 'Author']\n",
    "texts = texts[cols_to_keep]\n",
    "\n",
    "# Vérification\n",
    "print(\"New shape after column filtering:\", texts.shape)\n",
    "\n",
    "# Étape 2 : Remplir les NaN par la moyenne PAR CLASSE (par 'Author')\n",
    "# Cela suppose que toutes les colonnes sauf 'Author' sont numériques\n",
    "feature_cols = [col for col in texts.columns if col != 'Author']\n",
    "\n",
    "# On remplit les NaN par la moyenne des valeurs pour chaque auteur\n",
    "texts[feature_cols] = texts.groupby('Author')[feature_cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Vérification post-traitement\n",
    "print(\"Total remaining NaNs:\", texts.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc819cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de colonnes entièrement nulles ou égales à zéro : 0\n",
      "Shape finale du dataset : (14755, 28390)\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : supprimer les colonnes avec uniquement des NaN\n",
    "texts = texts.dropna(axis=1, how='all')\n",
    "\n",
    "# Étape 2 : supprimer les colonnes avec uniquement des zéros (sauf colonne 'Author')\n",
    "feature_cols = [col for col in texts.columns if col != 'Author']\n",
    "zero_cols = [col for col in feature_cols if (texts[col] == 0).all()]\n",
    "\n",
    "print(f\"Nombre de colonnes entièrement nulles ou égales à zéro : {len(zero_cols)}\")\n",
    "\n",
    "# Drop them\n",
    "texts.drop(columns=zero_cols, inplace=True)\n",
    "\n",
    "# Vérification\n",
    "print(\"Shape finale du dataset :\", texts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b521d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.to_parquet(\"../Data/stylo_embedds.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-paper-9dnSQuhV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
